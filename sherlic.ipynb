{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fourlang.text_to_4lang import TextTo4lang\n",
    "from fourlang.lexicon import Lexicon\n",
    "from graphviz import Source\n",
    "from scripts.parse_data import read_sherliic, build_graph\n",
    "from scripts.similarity import Similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "preds = []\n",
    "text_to_4lang = TextTo4lang(lang=\"en\")\n",
    "data = read_sherliic(\"data/dev.csv\", ud_path=\"data/relation_index.tsv\", keep_context=True)\n",
    "data_frame = build_graph(data)\n",
    "\n",
    "lexicon = Lexicon(lang=\"en\")\n",
    "\n",
    "similarity = Similarity(with_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['premise_text'] = data[\"prem_argleft\"] + \" \" + data[\"premise\"] + \" \" + data[\"prem_argright\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hyp_text'] = data[\"hypo_argleft\"] + \" \" + data[\"hypothesis\"] + \" \" + data[\"hypo_argright\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = text_to_4lang.process_text(\"An attempt to cause damage, injury to, or death of opponent or enemy.\", method=\"expand\", depth=0)\n",
    "dot_graph_premise = premise.to_dot()\n",
    "Source(dot_graph_premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def asim_jac_edges(graph_premise, graph_hypothesis):\n",
    "    \"\"\"\n",
    "    Asymmetric Jaccard similarity between the edges of the definition graphs\n",
    "    :param graph_premise: the definition graph of the premise\n",
    "    :param graph_hypothesis: the definition graph of the hypothesis\n",
    "    :return: the ratio of overlapping edges per the length of the hypothesis definition\n",
    "    \"\"\"\n",
    "    prem = set([(clear_node(s), clear_node(r), e['color'])\n",
    "                for (s, r, e) in graph_premise.G.edges(data=True)])\n",
    "    hyp = set([(clear_node(s), clear_node(r), e['color'])\n",
    "               for (s, r, e) in graph_hypothesis.G.edges(data=True)])\n",
    "    \n",
    "    hyp_cleared = []\n",
    "    for triplet in hyp:\n",
    "        if triplet[0] != \"A\" and  triplet[0] != \"B\" and triplet[1] != \"A\" and triplet[1] != \"B\":\n",
    "            hyp_cleared.append(triplet)\n",
    "            \n",
    "    hyp = set(hyp_cleared)\n",
    "    sim = hyp & prem\n",
    "    if not sim or len(hyp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        #return float(len(sim)) / math.sqrt(len(hyp))\n",
    "        #return len(sim)\n",
    "        return float(len(sim)) / len(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asim_jac_nodes(graph_premise, graph_hypothesis):\n",
    "    \"\"\"\n",
    "    Asymmetric Jaccard similarity between the nodes of the definition graphs\n",
    "    :param graph_premise: the definition graph of the premise\n",
    "    :param graph_hypothesis: the definition graph of the hypothesis\n",
    "    :return: the ratio of overlapping nodes per the length of the hypothesis definition\n",
    "    \"\"\"\n",
    "    prem = set([clear_node(node) for node in graph_premise.G.nodes])\n",
    "    hyp = set([clear_node(node) for node in graph_hypothesis.G.nodes])\n",
    "    \n",
    "    hyp_cleared = []\n",
    "    for triplet in hyp:\n",
    "        if triplet != \"a\" and  triplet != \"b\" and triplet != \"a\" and triplet != \"b\":\n",
    "            hyp_cleared.append(triplet)\n",
    "            \n",
    "    hyp = set(hyp_cleared)\n",
    "    sim = hyp & prem\n",
    "    if not sim or len(hyp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(len(sim)) / len(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clear_node(node):\n",
    "    \"\"\"\n",
    "    Clears the node from the 4lang id parts\n",
    "    :param node: the text to clear\n",
    "    :return: the cleared text\n",
    "    \"\"\"\n",
    "    return re.sub(r'_[0-9][0-9]*', '', node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "guesses = []\n",
    "for i in tqdm(range(len(data_frame))):\n",
    "    index = i\n",
    "    premise = data_frame[\"premise\"][index]\n",
    "    hypothesis = data_frame[\"hypothesis\"][index]\n",
    "    score = data.score[index]\n",
    "    graph_premise = text_to_4lang.process_deps(premise, method=\"expand\", depth=2, blacklist=[\"in\", \"on\", \"of\"], filt=False, black_or_white=\"\")\n",
    "    graph_hypothesis = text_to_4lang.process_deps(hypothesis, method=\"expand\", depth=1, blacklist=[\"in\", \"on\", \"of\"], filt=False, black_or_white=\"\")\n",
    "    pred = asim_jac_edges(graph_premise, graph_hypothesis)\n",
    "    guesses.append(pred)\n",
    "    if pred >= 0.1:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as pr\n",
    "bPrecis, bRecall, bFscore, bSupport = pr(data_frame.score.tolist(), preds)\n",
    "\n",
    "print(\"Precision: \" +  str(bPrecis[1]))\n",
    "print(\"Recall: \" +  str(bRecall[1]))\n",
    "print(\"Fscore: \" +  str(bFscore[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_4lang.get_definition(\"overtake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = data_frame.score.tolist()\n",
    "\n",
    "for i, score in enumerate(gold):\n",
    "    if preds[i] == 1 and score == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_premise = text_to_4lang.process_deps(data_frame.iloc[74].premise, method=\"expand\", depth=3, blacklist=[\"in\", \"on\", \"of\"])\n",
    "graph_hypothesis = text_to_4lang.process_deps(data_frame.iloc[74].hypothesis, method=\"expand\", depth=1, black_or_white=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_graph_premise = graph_premise.to_dot()\n",
    "Source(dot_graph_premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_graph_premise = graph_hypothesis.to_dot()\n",
    "Source(dot_graph_premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_jac_edges(graph_premise, graph_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_4lang.get_definition(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "67,68,72,75,76,95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(\"Printing float range with numpy.arange()\")\n",
    "\n",
    "thresholds = []\n",
    "print(\"Example one\")\n",
    "for i in numpy.arange(0, 1, 0.05):\n",
    "    thresholds.append(i)\n",
    "thresholds.append(1.0)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(\"Printing float range with numpy.arange()\")\n",
    "\n",
    "thresholds = []\n",
    "print(\"Example one\")\n",
    "for i in numpy.arange(0, 10, 0.5):\n",
    "    thresholds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recals = []\n",
    "f1_scores = []\n",
    "yields = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = []\n",
    "    for score in guesses:\n",
    "        if float(score) >= thresh:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    p = pr(data_frame.score.tolist(), preds)\n",
    "    precisions.append(p[0][1])\n",
    "    recals.append(p[1][1])\n",
    "    f1_scores.append(p[2][1])\n",
    "    yields.append(preds.count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "#Plotting to our canvas\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.plot(thresholds,precisions,label='precision',linewidth=3)\n",
    "plt.plot(thresholds,recals,label='recal',linewidth=3)\n",
    "plt.plot(thresholds,f1_scores,label='f1_score',linewidth=3)\n",
    "\n",
    "plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "#plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(\"2-1 expand n/N nodes\")\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "#Showing what we plotted, we can see we achieved pretty good values with ~0,62 f1_score and accuracy\n",
    "#Interesting thing to notice is that if we had some edge similarity, raising the threshold value doesnt change the result.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "#Plotting to our canvas\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.plot(thresholds,yields,label='yield',linewidth=3)\n",
    "\n",
    "#plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "#plt.xticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.yticks([50, 300, 1000])\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(\"4-2 expand n/N yields\")\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "#Showing what we plotted, we can see we achieved pretty good values with ~0,62 f1_score and accuracy\n",
    "#Interesting thing to notice is that if we had some edge similarity, raising the threshold value doesnt change the result.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.score.tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state\", \"r+\") as f:\n",
    "    dist_scores = []\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        pred = line[2]\n",
    "        dist_scores.append(1 if pred==\"True\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bPrecis, bRecall, bFscore, bSupport = pr(data_frame.score.tolist(), dist_scores)\n",
    "\n",
    "print(\"Precision: \" +  str(bPrecis[1]))\n",
    "print(\"Recall: \" +  str(bRecall[1]))\n",
    "print(\"Fscore: \" +  str(bFscore[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "for i in tqdm(range(len(data_frame))):\n",
    "    index = i\n",
    "    premise = data_frame[\"premise\"][index]\n",
    "    hypothesis = data_frame[\"hypothesis\"][index]\n",
    "    if preds[i] == 1 or dist_scores[i] == 1:\n",
    "        guesses.append(1)\n",
    "    else:\n",
    "        guesses.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bPrecis, bRecall, bFscore, bSupport = pr(data_frame.score.tolist(), guesses)\n",
    "\n",
    "print(\"Precision: \" +  str(bPrecis[1]))\n",
    "print(\"Recall: \" +  str(bRecall[1]))\n",
    "print(\"Fscore: \" +  str(bFscore[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_frame.iloc[2].premise[0]:\n",
    "    if i[0] == \"root\":\n",
    "        print(i[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for j in tqdm(range(len(data_frame))):\n",
    "    index = j\n",
    "    \n",
    "    for i in data_frame.iloc[index].premise[0]:\n",
    "        if i[0] == \"root\":\n",
    "            premise = i[2][0]\n",
    "            \n",
    "    for i in data_frame.iloc[index].hypothesis[0]:\n",
    "        if i[0] == \"root\":\n",
    "            hypothesis = i[2][0]\n",
    "    \n",
    "    score = data_frame.score[index]\n",
    "    \n",
    "    hyp_syn_names_all = []\n",
    "    hyper_premise_names_all = []\n",
    "    \n",
    "    premise_syns = wn.synsets(premise)\n",
    "    hyp_syns = wn.synsets(hypothesis)\n",
    "    \"\"\"\n",
    "    if len(premise_syns) > 0 and len(hyp_syns) > 0:\n",
    "        en_premise = premise_syns[0].lemmas()[0].name()\n",
    "        en_hyp = hyp_syns[0].lemmas()[0].name()\n",
    "        fourlang_score = get_4lang_score(en_premise, en_hyp)\n",
    "    else:\n",
    "        fourlang_score = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for premise_syn in premise_syns:\n",
    "\n",
    "        hyperpremise = set([i for i in premise_syn.closure(lambda s:s.hypernyms())])\n",
    "\n",
    "        hyper_premise_lemmas = []\n",
    "        for i in hyperpremise:\n",
    "            lemmas = i.lemmas()\n",
    "            for lemm in lemmas:\n",
    "                hyper_premise_lemmas.append(lemm)\n",
    "\n",
    "        hyper_premise_names = set([i.name() for i in hyper_premise_lemmas])\n",
    "        hyper_premise_names_all += list(hyper_premise_names)\n",
    "        \n",
    "    for hyp_syn in hyp_syns:\n",
    "        hyp_syn_lemmas = hyp_syn.lemmas()\n",
    "        hyp_syn_names = set([i.name() for i in hyp_syn_lemmas])\n",
    "        \n",
    "        hyp_syn_names_all += list(hyp_syn_names)\n",
    "    \n",
    "    if (set(hyp_syn_names_all) & set(hyper_premise_names_all)):\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as pr\n",
    "bPrecis, bRecall, bFscore, bSupport = pr(data_frame.score.tolist(), preds)\n",
    "\n",
    "print(\"Precision: \" +  str(bPrecis[1]))\n",
    "print(\"Recall: \" +  str(bRecall[1]))\n",
    "print(\"Fscore: \" +  str(bFscore[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda27c4385cd81d42338dc23456e05b5ed4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
